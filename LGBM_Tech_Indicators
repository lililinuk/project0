from finlab.data import Data
import talib
import numpy as np
import pandas as pd
import lightgbm
import seaborn as sns
import matplotlib.pyplot as plt


# import 15 min pircing data
data = Data()
twii = data.get("發行量加權股價指數")
twii = twii['台股指數']
twii = twii[ (twii.index.minute % 5 == 0) & (twii.index.second == 0)]

# set up features

sma = talib.SMA(twii, timeperiod=550)
wma = talib.WMA(twii, timeperiod=550)
mom = talib.MOM(twii, timeperiod=550)
k, d = talib.STOCH  (twii, twii, twii, fastk_period=120, slowk_period=60, slowd_period=60)
k2, d2 = talib.STOCH(twii, twii, twii, fastk_period=240, slowk_period=120, slowd_period=120)
k3, d3 = talib.STOCH(twii, twii, twii, fastk_period=360, slowk_period=180, slowd_period=180)
k4, d4 = talib.STOCH(twii, twii, twii, fastk_period=480, slowk_period=240, slowd_period=240)
k5, d5 = talib.STOCH(twii, twii, twii, fastk_period=640, slowk_period=320, slowd_period=320)
k6, d6 = talib.STOCH(twii, twii, twii, fastk_period=720, slowk_period=360, slowd_period=360)
k7, d7 = talib.STOCH(twii, twii, twii, fastk_period=840, slowk_period=420, slowd_period=420)
k8, d8 = talib.STOCH(twii, twii, twii, fastk_period=960, slowk_period=480, slowd_period=480)

rsi = talib.RSI (twii, timeperiod=550)
rsi2 = talib.RSI(twii, timeperiod=1100)
rsi3 = talib.RSI(twii, timeperiod=1650)
rsi4 = talib.RSI(twii, timeperiod=2200)
rsi5 = talib.RSI(twii, timeperiod=2750)
rsi6 = talib.RSI(twii, timeperiod=3300)



macd1, macd12, macd13 = talib.MACD(twii, fastperiod=120, slowperiod=60, signalperiod=60)
macd2, macd22, macd23 = talib.MACD(twii, fastperiod=240, slowperiod=120, signalperiod=120)
macd3, macd32, macd33 = talib.MACD(twii, fastperiod=360, slowperiod=180, signalperiod=180)
macd4, macd42, macd43 = talib.MACD(twii, fastperiod=480, slowperiod=240, signalperiod=240)
macd5, macd52, macd53 = talib.MACD(twii, fastperiod=720, slowperiod=300, signalperiod=300)
macd6, macd62, macd63 = talib.MACD(twii, fastperiod=660, slowperiod=1430, signalperiod=495)

#willr = talib.WILLR(twii, twii, twii, timeperiod=120)
#cci = talib.CCI(twii, twii, twii, timeperiod=120)

dataset = pd.DataFrame({
    'RSIb': rsi / 50,
    'RSIb2': rsi2 / 50,
    'RSIb3': rsi3 / 50,
    'RSIb4': rsi4 / 50,
    'RSIb5': rsi5 / 50,
    'RSIb6': rsi6 / 50,
    'MOMb': mom - 0,
    'KDb': k - d,
    'KDb2': k2 - d2,
    'KDb3': k3 - d3,
    'KDb4': k4 - d4,
    'KDb5': k5 - d5,
    'KDb6': k6 - d6,
    'KDb7': k7 - d7,
    'KDb8': k8 - d8,
    
    'MACD1' : macd13,
    'MACD2' : macd23,
    'MACD3' : macd33,
    'MACD4' : macd43,
    'MACD5' : macd53,
    'MACD6' : macd63,
    
    'a5':   (twii.rolling(5).mean()   / twii),
    'a10':  (twii.rolling(10).mean()  / twii),
    'a20':  (twii.rolling(20).mean()  / twii),
    'a40':  (twii.rolling(40).mean()  / twii),
    'a80':  (twii.rolling(80).mean()  / twii),
    'a160': (twii.rolling(160).mean() / twii),
    'a320': (twii.rolling(320).mean() / twii),
    'a640': (twii.rolling(640).mean() / twii),
    'a720': (twii.rolling(720).mean() / twii),
    'a840': (twii.rolling(840).mean() / twii),
    'a960': (twii.rolling(960).mean() / twii),
    'a1024':(twii.rolling(1024).mean() / twii),
    
    'b1': twii/twii.shift(50),
    'b2': twii/twii.shift(100),
    'b3': twii/twii.shift(150),
    'b4': twii/twii.shift(200),
    'b5': twii/twii.shift(250),
    'b6': twii/twii.shift(300),
    'b7': twii/twii.shift(350),
    
    'LINEARREG_SLOPE0': talib.LINEARREG_SLOPE(twii, 550),
    'LINEARREG_SLOPE1': talib.LINEARREG_SLOPE(twii, 1100),

    'ADXR0': talib.ADXR(twii, twii, twii, 60),
    'ADXR1': talib.ADXR(twii, twii, twii, 120),
    'ADXR2': talib.ADXR(twii, twii, twii, 240),
    'ADXR3': talib.ADXR(twii, twii, twii, 360),
    'ADXR4': talib.ADXR(twii, twii, twii, 480),
    'ADXR5': talib.ADXR(twii, twii, twii, 640),
    'ADXR6': talib.ADXR(twii, twii, twii, 1280),

    'return': twii.shift(-10) > twii,
})

feature_names = list(dataset.columns[:-1])



# train data

dataset_train = dataset[:'2015']
dataset_test = dataset['2016':]

cf = lightgbm.LGBMClassifier(boosting_type='gbdt', class_weight=None,
               colsample_bytree=0.9501241488957805, importance_type='split',
               learning_rate=0.05, max_depth=-1, min_child_samples=301,
               min_child_weight=0.1, min_split_gain=0.0, n_estimators=1000,
               n_jobs=-1, num_leaves=28, objective=None, random_state=None,
               random_stats=5, reg_alpha=0, reg_lambda=100, silent=True,
               subsample=0.9326466073236168, subsample_for_bin=200000,
               subsample_freq=0)
cf.fit(dataset_train[feature_names],dataset_train['return'])
cf.score(dataset_test[feature_names],dataset_test['return'])

# plot important feature

importance = pd.DataFrame(zip(feature_names, cf.feature_importances_), columns = ['feature names', 'importance'])
importance = importance.sort_values('importance', ascending = False)


%matplotlib inline
plt.figure(figsize = (20,10))
sns.barplot( x = 'importance',y = 'feature names',data = importance)





## finding the best estimators ,the following code ran and then get the input of best estimators 

# from scipy.stats import randint as sp_randint
# from scipy.stats import uniform as sp_uniform
# from sklearn.model_selection import RandomizedSearchCV, GridSearchCV
# train = dataset_train[feature_names] , dataset_train['return'] > 1
# test = dataset_test[feature_names] , dataset_test['return'] > 1 

# fit_params={"early_stopping_rounds":30, 
#             "eval_metric" : 'auc', 
#             "eval_set" : [test],
#             'eval_names': ['valid'],
#             'verbose': 100,
#             'categorical_feature': 'auto'}


# param_test ={'num_leaves': sp_randint(6, 50), 
#              'min_child_samples': sp_randint(100, 500), 
#              'min_child_weight': [1e-5, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4],
#              'subsample': sp_uniform(loc=0.2, scale=0.8), 
#              'colsample_bytree': sp_uniform(loc=0.4, scale=0.6),
#              'reg_alpha': [0, 1e-1, 1, 2, 5, 7, 10, 50, 100],
#              'reg_lambda': [0, 1e-1, 1, 5, 10, 20, 50, 100]}

# n_HP_points_to_test = 100





# gs = RandomizedSearchCV(
#     estimator=cf, param_distributions=param_test, 
#     n_iter=n_HP_points_to_test,
#     #scoring='roc_auc',
#     cv=3,
#     refit=True,
#     random_state=314,
#     verbose=True)

# gs.fit(*train, **fit_params)


# # get the best model
# gs.best_estimator_



================# #finding the best estimators =============================================

# backtesting 
prediction = cf.predict_proba(dataset[feature_names])
prediction = pd.Series(prediction.swapaxes(0,1)[1],index = dataset.index)

twii_dataset = twii[dataset.index]
gains = (twii_dataset.shift(-1) - twii_dataset)
signal = (prediction  > prediction.quantile(0.7)).rolling(10).sum() > 0
twii_dataset = (gains[signal]['2016':]).cumsum()
twii_dataset.plot()

# transaction fees
signal['2016':].astype(int).diff().abs().sum()



# randomly generate numbers of "hold_count" days of holing and calculate reutrns 
hold_count = signal['2016':].sum()
import numpy as np
samples = []
for i in range(1000):
    ret =  (np.random.choice(gains['2016':].dropna(),hold_count).sum())
    samples.append(ret)
    
pd.Series(samples).hist()

(pd.Series(samples) > 2500).sum() / len(samples)

